{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn explain\n",
    "nn has two main parts : data and model components\n",
    "\n",
    "\n",
    "containers are responsible for model components and parameters/buffers are responsible for model data\n",
    "\n",
    "\n",
    "containers : Module, Sequential, ModuleList, ModuleDict, ParameterList, ParameterDict for module construction\n",
    "\n",
    "\n",
    "parameters : parameter(...) for model training\n",
    "\n",
    "buffers    : parameter(...) for model aux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.parameters and buffers\n",
    "In one model, \n",
    "\n",
    "parameter needs to backward and be updated by optimizer.step\n",
    "\n",
    "\n",
    "buffer needs to be used in backward but not be updated by optimizer.step\n",
    "\n",
    "\n",
    "both of these data are responsible for the whole module, thus they would be saved by model.state_dict() in form of OrderDict. Moreover, they would be loaded by model.load_state_dict(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Parameter(...) should be used in the __init__ function in order to have init para at the first place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not added in nn.Module parameters : tensor([[0.4228]], requires_grad=True)\n",
      "test(\n",
      "  (linear): Linear(in_features=4, out_features=5, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.7117,  1.8141,  0.3526, -1.7719],\n",
      "        [-0.3638, -0.7159,  0.6690,  1.6504],\n",
      "        [ 1.6786, -1.1447,  1.5435, -1.0639],\n",
      "        [-0.7800, -0.3764, -0.5309,  0.7988]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0989, -0.0407, -0.4824, -0.1058],\n",
      "        [-0.1064,  0.3533, -0.4064, -0.3245],\n",
      "        [ 0.2952,  0.4070, -0.0780, -0.0521],\n",
      "        [-0.1896, -0.1403,  0.1634,  0.2207],\n",
      "        [ 0.0987,  0.2991,  0.4449, -0.1054]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4262,  0.3874, -0.4072, -0.2242, -0.1540], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test, self).__init__()\n",
    "        self.a = nn.Parameter(torch.randn(4,4))\n",
    "        self.linear = nn.Linear(4,5)\n",
    "        self.tensor_test = torch.rand((1,1), requires_grad=True)\n",
    "        print(\"Not added in nn.Module parameters : {}\".format(self.tensor_test))\n",
    "model = test()\n",
    "print(model)\n",
    "for para in model.parameters():\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====para=====\n",
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "=====buff=====\n",
      "tensor(3.)\n",
      "=====orderlist=====\n",
      "OrderedDict([('param1', tensor(1.)), ('param2', tensor(2.)), ('buffer', tensor(3.))])\n",
      "=====save&load=====\n",
      "OrderedDict([('param1', tensor(1.)), ('param2', tensor(2.)), ('buffer', tensor(3.))])\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        \n",
    "        # fist way to have a parameter\n",
    "        # self.x = nn.Parameter(...) directly add one var into OrderDict\n",
    "        self.param1 = nn.Parameter(torch.tensor(1.))\n",
    "        \n",
    "        # second way to have a parameter\n",
    "        # x = nn.Parameter(...) and self.register_parameter() in order to add normal parameter into OrderDict \n",
    "        param2 = nn.Parameter(torch.tensor(2.))\n",
    "        self.register_parameter('param2', param2)\n",
    "        \n",
    "        # the only way to have buffer\n",
    "        # self.register_buffer in order to add normal tensor into OrderDict\n",
    "        buff = torch.tensor(3.)\n",
    "        self.register_buffer('buffer', buff)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ParameterList can act as an iterable, or be indexed using ints\n",
    "        x = self.param1\n",
    "        y = self.param2\n",
    "        z = torch.mm(x,y)\n",
    "        return z\n",
    "\n",
    "model = MyModule()\n",
    "print(\"=====para=====\")\n",
    "for para in model.parameters():\n",
    "    print(para)\n",
    "\n",
    "print(\"=====buff=====\")\n",
    "for buff in model.buffers():\n",
    "    print(buff)\n",
    "\n",
    "print(\"=====orderlist=====\")\n",
    "print(model.state_dict())\n",
    "\n",
    "print(\"=====save&load=====\")\n",
    "# save model and load\n",
    "PATH = './MyModule_dict'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "model2 = MyModule()\n",
    "model2.load_state_dict(torch.load(PATH))\n",
    "print(model2.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. containers include Module, Sequential, ModuleList, ModuleDict, ParameterList, ParameterDict\n",
    "\n",
    "Among them, nn.Module is the father class and the five following classes should be put under nn.Module class.\n",
    "\n",
    "\n",
    "These containers can be used for adding module components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1272,  0.0485, -0.2189,  1.1651, -0.3613, -1.0465,  0.2440, -0.3423,\n",
      "         -0.1392,  1.2922],\n",
      "        [ 0.7822, -0.4773, -0.0313, -0.0306, -0.6284,  2.0574,  0.4001,  1.1377,\n",
      "          2.6969, -0.6150],\n",
      "        [-0.2943, -0.2042, -0.0590, -0.7561, -0.3240, -0.5563,  1.8937, -0.0426,\n",
      "          0.8770, -0.5655],\n",
      "        [-1.8932, -0.0217, -0.1494, -0.8232, -0.1276, -1.1098, -2.0809, -0.4547,\n",
      "         -2.0309, -1.6300],\n",
      "        [-0.9356, -0.6837, -0.2891,  0.5182, -1.8027, -0.6758, -0.9304,  0.6134,\n",
      "          0.5430, -0.4894],\n",
      "        [-0.2890,  0.5644, -1.1721, -0.2471,  0.5923,  0.8998,  0.4963, -0.9848,\n",
      "          0.2470,  0.5051],\n",
      "        [ 1.5218,  0.3686,  1.8202, -2.0923, -0.0528,  0.8303,  1.5411, -1.0907,\n",
      "         -0.0340, -0.7888],\n",
      "        [ 1.9285,  0.6090,  0.2700, -1.5023,  0.4229,  1.2153,  0.3863, -0.0705,\n",
      "          0.3639, -0.1447],\n",
      "        [ 1.2194,  0.1338, -0.3339, -0.4477, -1.6526,  0.2889,  0.1432, -0.9682,\n",
      "          1.7001, -0.2439],\n",
      "        [-1.0656, -0.0360, -0.2370, -0.5426,  0.3353, -0.9458, -0.5447,  0.3246,\n",
      "         -0.7897,  0.5933]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.2897, -1.3011, -1.5584, -0.0693, -1.1209,  0.4925, -0.8137,  1.2610,\n",
      "         -0.2423,  2.0114],\n",
      "        [ 0.9660,  1.2482, -1.2639,  0.6632, -0.0725,  1.4572,  0.3658,  0.5397,\n",
      "         -0.3138, -0.3719],\n",
      "        [ 2.0419,  0.0149,  0.2129, -1.1416, -0.0395, -0.5777, -1.6009,  0.8897,\n",
      "          0.3331,  0.6623],\n",
      "        [ 0.4827, -0.0346,  1.4666, -0.2000,  1.2827, -0.7671, -0.1858, -0.0996,\n",
      "          0.5675,  0.5138],\n",
      "        [ 1.1788, -0.0626, -1.7979, -1.1787, -1.7680,  0.4960, -1.5570,  0.3535,\n",
      "         -0.3616, -1.5292],\n",
      "        [ 0.1304,  0.4080,  0.9903, -1.2439,  0.5520,  0.8561, -0.2041,  0.5669,\n",
      "         -1.0943,  0.3526],\n",
      "        [ 0.9992,  0.0460,  1.2301, -0.3052,  1.5828,  1.5228, -0.0661,  0.7809,\n",
      "          1.5005, -0.4885],\n",
      "        [-2.7863,  0.6234, -0.6326,  1.3967,  0.8588,  0.9872,  1.1053,  0.0458,\n",
      "         -1.0755,  0.9800],\n",
      "        [ 1.4134,  1.0331, -0.0689,  0.0761,  0.1047,  0.3433, -0.4679,  1.7971,\n",
      "          0.3781,  1.9330],\n",
      "        [-0.1699,  0.7251,  0.6727,  1.0252,  0.3495, -0.9292,  0.3455, -0.1316,\n",
      "         -1.8527,  0.4031]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3315, -0.5321, -1.8555, -0.6891,  0.1775,  2.1232,  0.3400, -0.4947,\n",
      "          1.0529,  0.8292],\n",
      "        [ 0.6971,  0.2155, -1.2420,  1.8929,  0.1021, -1.5533, -0.7138,  0.4380,\n",
      "          0.2705, -0.9283],\n",
      "        [-1.7805, -1.3508,  1.6459, -1.0729,  1.2035, -0.2043, -0.1776, -0.1222,\n",
      "          1.3262, -0.5616],\n",
      "        [ 2.3524,  2.2041, -0.8371,  0.1121,  0.5186,  0.5753,  1.7528, -0.4266,\n",
      "         -1.0175, -1.5005],\n",
      "        [-1.5494,  1.4152, -0.8769, -0.1506, -1.0519, -1.9525, -2.4313,  0.2017,\n",
      "         -0.5876,  1.0076],\n",
      "        [ 1.3808, -1.3536,  0.2144,  0.4510,  1.4110, -0.2481,  0.9671,  0.1991,\n",
      "         -0.1174, -0.8905],\n",
      "        [ 0.0621,  1.7080, -0.3653, -1.5411, -0.4927, -0.4560, -1.6211, -0.7542,\n",
      "          1.8484,  0.6268],\n",
      "        [-1.3514, -0.3148,  0.5298,  1.2027,  0.1272,  0.7550, -0.9444,  0.2315,\n",
      "          0.9724,  0.8575],\n",
      "        [-0.0159, -0.3275,  0.2067, -0.5979,  1.0587,  0.7018,  0.5198, -1.6832,\n",
      "         -1.2451, -2.4252],\n",
      "        [ 0.1981, -0.2518, -0.9602, -0.1487,  0.8207, -0.5016,  0.6196, -1.2132,\n",
      "          1.8092, -0.3554]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 6.7962e-01, -2.8898e-01,  1.1589e+00,  2.8584e-01,  3.1079e-01,\n",
      "         -6.8868e-02,  7.6685e-01,  5.8181e-01,  4.5243e-01, -1.4294e-01],\n",
      "        [ 1.2083e-01,  4.0153e-01, -1.4832e+00,  1.5304e+00, -2.5739e+00,\n",
      "         -8.7487e-01,  1.0981e+00, -4.3428e-01,  8.5398e-01, -1.8345e+00],\n",
      "        [ 1.4438e+00, -1.2628e-01,  8.2196e-01, -1.2117e-01, -5.0667e-04,\n",
      "          1.2031e+00, -1.5967e+00, -3.2087e-01, -4.1998e-01, -8.3284e-01],\n",
      "        [-5.4858e-01,  8.0221e-01,  2.0471e+00,  1.6748e-01, -2.0367e+00,\n",
      "         -3.3429e-01, -1.8428e+00, -5.0920e-01,  9.8832e-01, -1.9312e+00],\n",
      "        [ 2.2597e+00,  1.1551e+00,  1.0640e+00, -1.1641e+00, -5.2413e-03,\n",
      "          4.5845e-01,  1.2414e+00, -5.3295e-01, -1.8846e-01, -1.1999e+00],\n",
      "        [ 2.1026e-01, -1.6540e+00,  1.3782e-01, -1.9963e-01,  2.8230e-01,\n",
      "          5.0834e-02, -2.4113e+00, -5.4577e-01,  4.0630e-01,  9.1583e-01],\n",
      "        [-5.4910e-01,  1.3679e+00, -7.0200e-01, -1.1392e+00,  1.5961e+00,\n",
      "          3.4773e-01, -2.5374e-01, -1.0742e+00, -4.7624e-02, -7.8659e-01],\n",
      "        [ 2.5541e+00, -1.6430e+00,  1.0651e+00,  2.2559e-01,  2.0944e-01,\n",
      "          3.6138e-01, -6.9031e-01,  1.5785e-01,  9.5121e-01,  1.4750e+00],\n",
      "        [ 2.4128e-01,  3.6578e-01, -1.2920e+00, -8.9485e-01, -1.7336e+00,\n",
      "          1.5655e-01,  2.8389e-01,  1.6320e+00,  4.5982e-01,  2.8155e-01],\n",
      "        [-6.1948e-01,  3.5572e-01,  1.2410e+00,  3.5135e-01,  1.4540e+00,\n",
      "         -6.7017e-01, -6.2489e-01, -3.7482e-02, -7.3168e-01, -9.7334e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.6729, -0.2239,  0.5050, -1.1456,  0.3482, -0.5312, -0.5992, -0.8990,\n",
      "         -1.2439,  0.4118],\n",
      "        [ 0.4025,  0.0817,  1.0454,  0.0439,  0.0039, -0.1642, -0.0220,  0.7715,\n",
      "         -1.5536,  0.6059],\n",
      "        [-0.9813,  0.8201,  1.0829,  0.9065, -0.9727, -0.4288,  1.0715,  1.0176,\n",
      "         -0.4217, -0.2285],\n",
      "        [-0.3859,  0.2431, -0.2646,  0.4911, -1.3625,  0.0747, -0.2332,  2.0955,\n",
      "         -0.7270, -2.6732],\n",
      "        [-1.0102,  1.7796,  0.7852,  0.5013, -0.2271,  0.2836, -0.3196, -0.3755,\n",
      "          0.3044, -1.7529],\n",
      "        [-0.7259, -0.6348,  0.0251, -0.4716,  0.9382,  1.1055,  1.7892,  1.3724,\n",
      "         -0.0602,  1.8223],\n",
      "        [ 0.9476,  1.5983, -0.2359,  0.6884, -1.1131,  1.5549, -0.1987, -0.4774,\n",
      "          1.6887,  0.2196],\n",
      "        [ 0.5855,  1.4086,  0.6228, -0.9929, -0.6039,  0.1025,  0.6259,  0.0567,\n",
      "         -1.1819, -1.2920],\n",
      "        [-1.0040,  0.6635, -1.0416, -0.0063,  1.3749, -1.5630,  2.4231,  0.7641,\n",
      "         -1.0074, -2.7830],\n",
      "        [-1.2513,  1.5281, -1.9152, -0.1276, -0.7450, -0.8117, -1.7560,  0.7225,\n",
      "          1.0249,  0.9883]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2267, -0.2988,  1.2759,  0.6034,  0.5156, -0.7324,  0.6829,  0.5412,\n",
      "          1.1138,  0.5939],\n",
      "        [-2.1698,  0.6387, -0.5625, -0.5676, -0.3712,  1.6179,  0.8898, -0.1776,\n",
      "          0.9782, -1.0332],\n",
      "        [ 0.0814, -0.0798,  1.2036,  0.3521, -0.6993,  0.1678, -0.4531,  0.9286,\n",
      "         -0.4242,  0.4413],\n",
      "        [ 0.4022, -0.3517, -0.2270, -1.0313, -1.9118,  0.6313, -1.4309,  0.3935,\n",
      "          0.6558,  0.7470],\n",
      "        [-2.3720,  1.1339, -1.0378,  1.0519, -0.9055, -0.8738, -2.1823, -0.0150,\n",
      "          0.1516, -0.2144],\n",
      "        [ 1.6356,  1.5397, -0.2472, -0.9089,  0.2282, -1.3579, -0.6551,  0.2783,\n",
      "          0.3972, -0.2484],\n",
      "        [-0.2149,  0.0387,  0.0665,  0.1337,  0.9450, -1.3398, -1.3444, -0.7059,\n",
      "         -0.4305,  0.4096],\n",
      "        [ 2.1551,  0.7197,  1.3916,  1.3992,  2.3794, -0.3712, -0.5316,  0.2442,\n",
      "          0.6921,  0.1789],\n",
      "        [-0.7532, -0.4046,  0.7694, -0.5654, -0.4075,  1.2390, -0.1117,  1.0342,\n",
      "         -1.8529,  1.9393],\n",
      "        [-1.2164, -1.1084, -0.4600,  0.3755,  0.2265,  0.2649,  1.4187, -0.8829,\n",
      "          0.1351,  1.3328]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.8277,  0.8902,  0.5626,  0.4004, -0.4910, -0.6964,  0.1087,  1.4104,\n",
      "          0.8922, -0.9158],\n",
      "        [ 0.4419, -1.1163, -1.6775, -1.0703,  1.2202, -1.9728,  1.1004,  0.4496,\n",
      "          0.3696,  1.6107],\n",
      "        [-1.2348, -0.3070,  1.4153, -0.8673, -0.7127, -1.8589, -1.4880,  0.7539,\n",
      "          0.4692, -0.4995],\n",
      "        [ 0.7942, -0.9434,  0.7533, -1.8465, -0.5494, -0.0822, -0.9958, -1.5986,\n",
      "         -1.0323, -0.3087],\n",
      "        [ 0.9321, -0.6128,  0.2772, -1.5069, -0.8380,  0.1015, -0.3074, -0.4156,\n",
      "          0.1797, -1.3516],\n",
      "        [-0.5867, -1.5943,  0.5255,  1.7264, -0.2347, -0.8139, -0.8765,  1.4360,\n",
      "          0.6229, -1.1382],\n",
      "        [-0.7804,  1.7256, -2.6835,  0.4410, -0.0206, -2.0391,  0.1557, -1.2903,\n",
      "         -0.5063, -0.4872],\n",
      "        [-0.8509, -0.9069,  2.2438,  0.4538, -0.1104,  1.1682, -0.9125, -0.8630,\n",
      "         -0.0386,  0.5237],\n",
      "        [-0.5481,  1.0986,  0.7209, -1.9107,  2.1426,  0.2947,  1.4956, -0.6600,\n",
      "         -0.7877,  0.2884],\n",
      "        [ 1.6816,  0.0564, -1.0231, -0.9418,  0.9261,  0.3748, -1.3193, -0.3569,\n",
      "         -0.4350, -0.9372]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-9.0306e-01, -5.7304e-01, -6.3238e-01,  8.8665e-01, -1.0025e+00,\n",
      "         -5.8412e-02, -1.7472e-01, -3.4795e-01, -8.7268e-01, -1.0835e-01],\n",
      "        [-8.3638e-01,  1.6187e-01,  2.3460e-01,  1.1704e+00,  1.3063e+00,\n",
      "          1.6463e+00, -5.2217e-02, -4.2500e-01,  8.9235e-01,  9.9901e-01],\n",
      "        [ 3.4499e-01, -4.1781e-01,  3.6023e-01,  1.4980e+00,  1.8332e-01,\n",
      "          6.1424e-01,  6.0575e-01, -6.5435e-01,  2.4985e-01,  1.8388e-01],\n",
      "        [-8.8655e-01, -6.9191e-01,  1.2695e+00, -5.8196e-01, -1.6250e-01,\n",
      "         -1.4892e-01, -6.8814e-01,  1.6988e-01,  5.4392e-01,  5.3178e-01],\n",
      "        [-1.6535e+00, -1.9454e+00,  4.3875e-01, -1.6911e+00,  7.5137e-01,\n",
      "          3.3676e-01, -2.3101e-01, -1.1015e+00,  1.6203e+00, -1.0120e+00],\n",
      "        [ 2.3302e-01,  7.0348e-01, -4.5856e-01, -7.3033e-01,  1.8400e+00,\n",
      "         -3.9256e-01, -1.6624e+00,  1.1967e+00, -6.7993e-01, -1.1454e+00],\n",
      "        [ 2.9823e-01, -7.1550e-02,  5.9501e-01,  5.4327e-01, -1.0223e+00,\n",
      "          1.3854e+00, -6.7764e-01, -2.2576e-03, -3.5908e-01, -1.0442e+00],\n",
      "        [-1.8166e-01,  6.4142e-01, -3.1862e-01, -6.3947e-02,  3.2669e-01,\n",
      "          1.0641e+00,  5.9765e-02, -8.3977e-01, -3.4050e-01, -3.6869e-01],\n",
      "        [ 7.6353e-01,  1.9731e+00, -7.2661e-01, -1.4001e+00,  8.7829e-02,\n",
      "          6.2835e-01, -2.0153e+00, -2.1703e-01,  4.0696e-01,  2.7488e-01],\n",
      "        [-3.7591e-01, -9.5117e-01,  4.8948e-01,  2.7844e+00, -6.5856e-01,\n",
      "          1.1362e-02,  3.2077e-01,  1.2871e+00,  1.9109e-02,  4.3374e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2454, -0.0476,  0.2516, -0.2853,  0.7883, -1.3576, -0.9347,  2.5108,\n",
      "         -0.2680, -1.6712],\n",
      "        [ 0.1508, -1.4282, -0.2046, -0.2153,  0.3169, -0.9601,  1.3390, -1.4258,\n",
      "         -0.2284,  0.8178],\n",
      "        [ 0.2983,  2.2085,  1.2053, -0.2136,  1.7040, -0.2443,  0.8798, -1.3919,\n",
      "          1.0714, -1.0713],\n",
      "        [-0.5355, -0.0704, -0.9994,  1.0166, -0.2196, -0.2764, -0.9559, -0.0126,\n",
      "          0.2307, -0.9657],\n",
      "        [ 0.2852, -1.5754,  1.0704, -1.8282, -0.0927,  0.4007, -2.0028,  1.0541,\n",
      "         -0.7382,  1.2115],\n",
      "        [ 0.8782,  0.5516, -0.4345,  0.0905, -1.2947, -0.1918,  1.7856, -0.1709,\n",
      "         -0.9691,  0.0662],\n",
      "        [-1.4539, -2.7167, -1.1108,  0.0136,  0.4170, -0.4164,  0.7318,  0.7505,\n",
      "          0.4648,  1.1725],\n",
      "        [-1.6197, -0.0985,  0.3123, -0.5923,  0.9553, -1.0801, -1.3062,  0.9596,\n",
      "         -1.5984,  0.9388],\n",
      "        [ 3.1648,  0.0823, -1.5527,  0.0100,  0.8137,  0.8893,  0.4362, -1.3602,\n",
      "         -1.4273,  1.2668],\n",
      "        [ 1.2542,  0.9982, -1.0015,  1.5251, -1.7314, -0.1238,  0.1866,  0.4881,\n",
      "         -0.4569,  0.3543]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.7325,  0.1692,  0.5741,  1.2764, -0.7306, -1.1062, -0.1950,  1.0395,\n",
      "          0.1636,  1.8312],\n",
      "        [ 0.1126,  1.0121, -0.1621, -0.6816,  0.0076, -0.8879, -0.7347, -0.6920,\n",
      "          0.5953,  1.5906],\n",
      "        [-1.0077,  0.2171,  1.2208, -0.2922,  0.8943,  0.7935, -1.6906,  0.0107,\n",
      "          0.3364, -0.4518],\n",
      "        [ 0.0391,  1.5731, -1.6397,  2.2243,  1.4959, -0.3001,  0.2742,  0.8538,\n",
      "         -0.7207, -0.9030],\n",
      "        [ 0.5603,  0.8698,  0.8331,  1.6499,  1.6208, -0.3519,  0.6790,  1.4195,\n",
      "         -1.1185, -0.3955],\n",
      "        [-1.3437,  2.1854, -0.0237,  0.2790, -0.2950,  0.6583,  0.1929, -0.6720,\n",
      "          0.5347,  0.6495],\n",
      "        [-0.3625,  1.1896,  0.3543,  0.2213,  0.8095,  0.6894,  0.6818,  1.5905,\n",
      "          0.4028,  2.2044],\n",
      "        [-0.7772,  0.3503,  0.8768, -0.5330,  1.2647, -0.0443,  1.1367,  1.9992,\n",
      "          0.0778, -1.7167],\n",
      "        [ 1.5763, -0.3431, -1.5604, -1.4168, -0.0141, -0.1893, -0.1019,  1.6984,\n",
      "         -1.0488,  1.6917],\n",
      "        [ 0.4096,  0.8420,  0.3007, -1.5016, -1.8182,  1.3064,  0.0652,  0.0669,\n",
      "          0.3772,  0.0335]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ParameterList can act as an iterable, or be indexed using ints\n",
    "        for i, p in enumerate(self.params):\n",
    "            x = self.params[i // 2].mm(x) + p.mm(x)\n",
    "        return x\n",
    "\n",
    "model = MyModule()\n",
    "for para in model.parameters():\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0917, -7.0917, -7.0917, -7.0917, -7.0917, -7.0917, -7.0917, -7.0917,\n",
       "         -7.0917, -7.0917],\n",
       "        [ 0.4745,  0.4745,  0.4745,  0.4745,  0.4745,  0.4745,  0.4745,  0.4745,\n",
       "          0.4745,  0.4745],\n",
       "        [-0.5410, -0.5410, -0.5410, -0.5410, -0.5410, -0.5410, -0.5410, -0.5410,\n",
       "         -0.5410, -0.5410],\n",
       "        [-1.1596, -1.1596, -1.1596, -1.1596, -1.1596, -1.1596, -1.1596, -1.1596,\n",
       "         -1.1596, -1.1596],\n",
       "        [-2.8603, -2.8603, -2.8603, -2.8603, -2.8603, -2.8603, -2.8603, -2.8603,\n",
       "         -2.8603, -2.8603]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "                'left': nn.Parameter(torch.randn(5, 10)),\n",
    "                'right': nn.Parameter(torch.randn(5, 10))\n",
    "        })\n",
    "\n",
    "    def forward(self, x, choice):\n",
    "        # torch.mm() a@b\n",
    "        # torch.mul() a*b\n",
    "        x = self.params[choice].mm(x)\n",
    "        return x\n",
    "model = MyModule()\n",
    "model(torch.ones((10,10)), 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0569]],\n",
       "\n",
       "         [[ 0.0111]],\n",
       "\n",
       "         [[ 0.3262]],\n",
       "\n",
       "         [[ 0.3203]],\n",
       "\n",
       "         [[ 0.2505]],\n",
       "\n",
       "         [[-0.1535]],\n",
       "\n",
       "         [[-0.0760]],\n",
       "\n",
       "         [[ 0.6562]],\n",
       "\n",
       "         [[-0.1165]],\n",
       "\n",
       "         [[-0.3123]]],\n",
       "\n",
       "\n",
       "        [[[-0.0569]],\n",
       "\n",
       "         [[ 0.0111]],\n",
       "\n",
       "         [[ 0.3262]],\n",
       "\n",
       "         [[ 0.3203]],\n",
       "\n",
       "         [[ 0.2505]],\n",
       "\n",
       "         [[-0.1535]],\n",
       "\n",
       "         [[-0.0760]],\n",
       "\n",
       "         [[ 0.6562]],\n",
       "\n",
       "         [[-0.1165]],\n",
       "\n",
       "         [[-0.3123]]],\n",
       "\n",
       "\n",
       "        [[[-0.0569]],\n",
       "\n",
       "         [[ 0.0111]],\n",
       "\n",
       "         [[ 0.3262]],\n",
       "\n",
       "         [[ 0.3203]],\n",
       "\n",
       "         [[ 0.2505]],\n",
       "\n",
       "         [[-0.1535]],\n",
       "\n",
       "         [[-0.0760]],\n",
       "\n",
       "         [[ 0.6562]],\n",
       "\n",
       "         [[-0.1165]],\n",
       "\n",
       "         [[-0.3123]]],\n",
       "\n",
       "\n",
       "        [[[-0.0569]],\n",
       "\n",
       "         [[ 0.0111]],\n",
       "\n",
       "         [[ 0.3262]],\n",
       "\n",
       "         [[ 0.3203]],\n",
       "\n",
       "         [[ 0.2505]],\n",
       "\n",
       "         [[-0.1535]],\n",
       "\n",
       "         [[-0.0760]],\n",
       "\n",
       "         [[ 0.6562]],\n",
       "\n",
       "         [[-0.1165]],\n",
       "\n",
       "         [[-0.3123]]],\n",
       "\n",
       "\n",
       "        [[[-0.0569]],\n",
       "\n",
       "         [[ 0.0111]],\n",
       "\n",
       "         [[ 0.3262]],\n",
       "\n",
       "         [[ 0.3203]],\n",
       "\n",
       "         [[ 0.2505]],\n",
       "\n",
       "         [[-0.1535]],\n",
       "\n",
       "         [[-0.0760]],\n",
       "\n",
       "         [[ 0.6562]],\n",
       "\n",
       "         [[-0.1165]],\n",
       "\n",
       "         [[-0.3123]]],\n",
       "\n",
       "\n",
       "        [[[-0.0569]],\n",
       "\n",
       "         [[ 0.0111]],\n",
       "\n",
       "         [[ 0.3262]],\n",
       "\n",
       "         [[ 0.3203]],\n",
       "\n",
       "         [[ 0.2505]],\n",
       "\n",
       "         [[-0.1535]],\n",
       "\n",
       "         [[-0.0760]],\n",
       "\n",
       "         [[ 0.6562]],\n",
       "\n",
       "         [[-0.1165]],\n",
       "\n",
       "         [[-0.3123]]],\n",
       "\n",
       "\n",
       "        [[[-0.0569]],\n",
       "\n",
       "         [[ 0.0111]],\n",
       "\n",
       "         [[ 0.3262]],\n",
       "\n",
       "         [[ 0.3203]],\n",
       "\n",
       "         [[ 0.2505]],\n",
       "\n",
       "         [[-0.1535]],\n",
       "\n",
       "         [[-0.0760]],\n",
       "\n",
       "         [[ 0.6562]],\n",
       "\n",
       "         [[-0.1165]],\n",
       "\n",
       "         [[-0.3123]]],\n",
       "\n",
       "\n",
       "        [[[-0.0569]],\n",
       "\n",
       "         [[ 0.0111]],\n",
       "\n",
       "         [[ 0.3262]],\n",
       "\n",
       "         [[ 0.3203]],\n",
       "\n",
       "         [[ 0.2505]],\n",
       "\n",
       "         [[-0.1535]],\n",
       "\n",
       "         [[-0.0760]],\n",
       "\n",
       "         [[ 0.6562]],\n",
       "\n",
       "         [[-0.1165]],\n",
       "\n",
       "         [[-0.3123]]],\n",
       "\n",
       "\n",
       "        [[[-0.0569]],\n",
       "\n",
       "         [[ 0.0111]],\n",
       "\n",
       "         [[ 0.3262]],\n",
       "\n",
       "         [[ 0.3203]],\n",
       "\n",
       "         [[ 0.2505]],\n",
       "\n",
       "         [[-0.1535]],\n",
       "\n",
       "         [[-0.0760]],\n",
       "\n",
       "         [[ 0.6562]],\n",
       "\n",
       "         [[-0.1165]],\n",
       "\n",
       "         [[-0.3123]]],\n",
       "\n",
       "\n",
       "        [[[-0.0569]],\n",
       "\n",
       "         [[ 0.0111]],\n",
       "\n",
       "         [[ 0.3262]],\n",
       "\n",
       "         [[ 0.3203]],\n",
       "\n",
       "         [[ 0.2505]],\n",
       "\n",
       "         [[-0.1535]],\n",
       "\n",
       "         [[-0.0760]],\n",
       "\n",
       "         [[ 0.6562]],\n",
       "\n",
       "         [[-0.1165]],\n",
       "\n",
       "         [[-0.3123]]]], grad_fn=<PreluBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.choices = nn.ModuleDict({\n",
    "                'conv': nn.Conv2d(10, 10, 3),\n",
    "                'pool': nn.MaxPool2d(3)\n",
    "        })\n",
    "        self.activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['prelu', nn.PReLU()]\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, choice, act):\n",
    "        x = self.choices[choice](x)\n",
    "        x = self.activations[act](x)\n",
    "        return x\n",
    "\n",
    "model = MyModule()\n",
    "model(torch.ones((10,10,3,3)), 'conv', 'prelu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.difference between nn.Sequential and nn.Modulelist\n",
    "both of them are subclasses of containers in torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential class stores sequential list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_net(\n",
      "  (seq): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class seq_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(seq_net, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "                   nn.Conv2d(1,20,5),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(20,64,5),\n",
    "                   nn.ReLU()\n",
    "                   )\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "model = seq_net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ModuleList can be used as list, all elements can be used as elements in the list, but the modules in the list are registered automatically to the whole net and the parameters are automatically put on the whole nn.Module model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modlist_net(\n",
      "  (modlist): ModuleList(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class modlist_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modlist_net, self).__init__()\n",
    "        self.modlist = nn.ModuleList([\n",
    "                       nn.Conv2d(1, 20, 5),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Conv2d(20, 64, 5),\n",
    "                       nn.ReLU()\n",
    "                       ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "model = modlist_net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff 1 : nn.ModuleList has no forward functions but nn.Sequential has default forward functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff 2 :  nn.Sequential can be named using OrderedDict but nn.ModuleList cannot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_net(\n",
      "  (seq): Sequential(\n",
      "    (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "class seq_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(seq_net, self).__init__()\n",
    "        self.seq = nn.Sequential(OrderedDict([\n",
    "                   ('conv1', nn.Conv2d(1,20,5)),\n",
    "                   ('relu1', nn.ReLU()),\n",
    "                   ('conv2', nn.Conv2d(20,64,5)),\n",
    "                   ('relu2', nn.ReLU())\n",
    "                   ]))\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "model = seq_net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff 3 : module in nn.ModuleList has no order, we can put modules in casual order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff 4 : we can use \"for\" for duplicate modules in nn.ModuleList."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modlist_net(\n",
      "  (modlist): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (5): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (6): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (7): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (8): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (9): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class modlist_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modlist_net, self).__init__()\n",
    "        self.modlist = nn.ModuleList([nn.Linear(10,10) for i in range(10)]\n",
    "\n",
    "                                    )\n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "model = modlist_net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Other APIs for nn.Module base class\n",
    "\n",
    "collect other APIs not mentioned in the above.\n",
    "\n",
    "\n",
    "train : effect Dropout & BatchNorm layers\n",
    "\n",
    "\n",
    "eval  : effect Dropout & BatchNorm layers ---> equivalent to self.train(false)\n",
    "\n",
    "\n",
    "requires_grad_ : change if autograd should record operations on parameters\n",
    "\n",
    "\n",
    "register_forward_pre_hook : be called every time before forward() is invoked\n",
    "\n",
    "\n",
    "register_forward_hook : be called every time when forward() is invoked\n",
    "\n",
    "\n",
    "named_parameters / named_buffers / named_modules / named_children \n",
    "parameters / buffers / modules / children\n",
    "\n",
    "add_module\n",
    "\n",
    "apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7096, 0.9641, 0.2971, 0.3942],\n",
      "        [0.1870, 0.7542, 0.4927, 0.4536],\n",
      "        [0.5679, 0.3549, 0.6074, 0.8638],\n",
      "        [0.1525, 0.8247, 0.1993, 0.7845]])\n",
      "Linear(in_features=2, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# when it comes to tensor we use requires_grad_() or requires_grad = False\n",
    "x = torch.rand((4,4))\n",
    "x.requires_grad_(False)\n",
    "x.requires_grad = False\n",
    "print(x)\n",
    "\n",
    "# when it comes to nn.Module we use requires_grad_() or requires_grad = False\n",
    "# this can be used for freezing parameters when fine tuning\n",
    "# because the grad would not be changed when passing requires_grad_(False) layers\n",
    "y = nn.Linear(2,2)\n",
    "y.requires_grad_(False)\n",
    "y.requires_grad = False\n",
    "print(y)\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ParameterList can act as an iterable, or be indexed using ints\n",
    "        for i, p in enumerate(self.params):\n",
    "            x = self.params[i // 2].mm(x) + p.mm(x)\n",
    "        return x\n",
    "\n",
    "model = MyModule()\n",
    "x = model(torch.ones((10,10)))\n",
    "model.requires_grad_(False)\n",
    "loss = torch.sum(x)\n",
    "loss.backward()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
