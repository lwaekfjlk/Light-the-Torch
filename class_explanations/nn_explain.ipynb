{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn explain\n",
    "nn has two main parts : data and model components\n",
    "\n",
    "\n",
    "containers are responsible for model components and parameters/buffers are responsible for model data\n",
    "\n",
    "\n",
    "containers : Module, Sequential, ModuleList, ModuleDict, ParameterList, ParameterDict for module construction\n",
    "\n",
    "\n",
    "parameters : parameter(...) for model training\n",
    "\n",
    "buffers    : parameter(...) for model aux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.parameters and buffers\n",
    "\n",
    "**parameter is just tensor with requires_grad=True and have their own space in model.parameters() and model ordered list**\n",
    "\n",
    "\n",
    "**buffer is just tensor with requires_grad=True/False and have their own space in model.buffers() and model ordered list**\n",
    "\n",
    "\n",
    "In one model, \n",
    "\n",
    "parameter needs to backward and be updated by optimizer.step\n",
    "\n",
    "\n",
    "buffer needs to be used in backward but not be updated by optimizer.step\n",
    "\n",
    "\n",
    "both of these data are responsible for the whole module, thus they would be saved by model.state_dict() in form of OrderDict. Moreover, they would be loaded by model.load_state_dict(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Parameter(...) should be used in the __init__ function in order to have init para at the first place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not added in nn.Module parameters : tensor([[0.8124]], requires_grad=True)\n",
      "test(\n",
      "  (linear): Linear(in_features=4, out_features=5, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-0.6988, -1.2292, -0.4337, -0.0189],\n",
      "        [-0.1070, -0.6613, -0.3620,  0.7364],\n",
      "        [-0.4378,  1.9396,  0.4627,  1.7673],\n",
      "        [-0.6651,  0.5262, -1.1829,  0.8627]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3213, -0.2416, -0.4556,  0.0478],\n",
      "        [-0.0612,  0.1238, -0.3602,  0.4339],\n",
      "        [ 0.1471, -0.1525, -0.3646,  0.1010],\n",
      "        [ 0.4652,  0.1764,  0.3794,  0.2159],\n",
      "        [ 0.4705, -0.1052,  0.1555,  0.4119]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1092,  0.3478, -0.1668, -0.4925,  0.1740], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test, self).__init__()\n",
    "        self.a = nn.Parameter(torch.randn(4,4))\n",
    "        self.linear = nn.Linear(4,5)\n",
    "        self.tensor_test = torch.rand((1,1), requires_grad=True)\n",
    "        print(\"Not added in nn.Module parameters : {}\".format(self.tensor_test))\n",
    "model = test()\n",
    "print(model)\n",
    "for para in model.parameters():\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "=====para=====\n",
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "=====buff=====\n",
      "tensor(3., requires_grad=True)\n",
      "=====orderlist=====\n",
      "OrderedDict([('param1', tensor(1.)), ('param2', tensor(2.)), ('buffer', tensor(3.))])\n",
      "=====save&load=====\n",
      "tensor(3., requires_grad=True)\n",
      "OrderedDict([('param1', tensor(1.)), ('param2', tensor(2.)), ('buffer', tensor(3.))])\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        \n",
    "        # fist way to have a parameter\n",
    "        # self.x = nn.Parameter(...) directly add one var into OrderDict\n",
    "        self.param1 = nn.Parameter(torch.tensor(1.))\n",
    "        \n",
    "        # second way to have a parameter\n",
    "        # x = nn.Parameter(...) and self.register_parameter() in order to add normal parameter into OrderDict \n",
    "        param2 = nn.Parameter(torch.tensor(2.))\n",
    "        self.register_parameter('param2', param2)\n",
    "        \n",
    "        # the only way to have buffer\n",
    "        # self.register_buffer in order to add normal tensor into OrderDict\n",
    "        buff = torch.tensor(3.)\n",
    "        self.register_buffer('buffer', buff)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ParameterList can act as an iterable, or be indexed using ints\n",
    "        x = self.param1\n",
    "        y = self.param2\n",
    "        z = torch.mm(x,y)\n",
    "        return z\n",
    "\n",
    "model = MyModule()\n",
    "print(\"=====para=====\")\n",
    "for para in model.parameters():\n",
    "    print(para)\n",
    "\n",
    "print(\"=====buff=====\")\n",
    "for buff in model.buffers():\n",
    "    print(buff)\n",
    "\n",
    "print(\"=====orderlist=====\")\n",
    "print(model.state_dict())\n",
    "\n",
    "print(\"=====save&load=====\")\n",
    "# save model and load\n",
    "PATH = './MyModule_dict'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "model2 = MyModule()\n",
    "model2.load_state_dict(torch.load(PATH))\n",
    "print(model2.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. containers include Module, Sequential, ModuleList, ModuleDict, ParameterList, ParameterDict\n",
    "\n",
    "Among them, nn.Module is the father class and the five following classes should be put under nn.Module class.\n",
    "\n",
    "\n",
    "These containers can be used for adding module components.\n",
    "\n",
    "\n",
    "**It is quite important to notice that nn supports nesting. Once there is one class from nn.Module, any nn.Linear or other nn.Module defined inside the nn.Module woulde automatically added to the whole nn.Module.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 3.4073e-01, -6.1802e-01,  2.0644e+00,  9.2256e-01, -1.3488e+00,\n",
      "          4.9499e-01,  5.7657e-01, -9.7584e-03, -1.2473e+00,  7.5066e-01],\n",
      "        [-2.0932e+00, -6.6143e-01,  5.4935e-01,  3.0891e-01, -9.4306e-01,\n",
      "         -2.2446e-01,  1.0764e+00,  2.2019e+00,  6.3417e-01,  5.4304e-01],\n",
      "        [ 5.3346e-02, -4.5061e-01, -1.0908e-01, -1.5221e+00,  5.8349e-01,\n",
      "         -6.2337e-01,  7.8563e-01, -1.4640e+00,  6.3888e-01,  4.1768e-01],\n",
      "        [ 1.8526e-01, -1.3341e+00, -1.8586e-01, -2.9592e-01, -7.3462e-04,\n",
      "         -1.5999e+00,  4.0736e-01, -6.1220e-01,  1.2491e+00,  1.1794e+00],\n",
      "        [ 1.1951e+00, -9.0779e-02, -1.1695e-03, -1.4847e+00, -1.8719e+00,\n",
      "          2.3298e-01, -3.7501e-02,  5.4963e-02, -2.4846e-01,  1.2055e+00],\n",
      "        [ 7.6458e-01,  1.1144e-01,  1.4895e+00,  1.9522e+00,  4.3273e-01,\n",
      "         -1.0283e+00,  8.5294e-01, -3.0160e-01, -1.4007e+00, -5.2178e-01],\n",
      "        [ 1.2018e+00,  3.4727e-01,  1.9874e+00, -1.4930e+00,  1.2953e+00,\n",
      "         -1.4587e+00,  2.0003e-01,  8.3552e-01,  5.9807e-01,  6.3946e-01],\n",
      "        [-1.8513e+00,  5.9169e-02, -1.1723e+00,  3.7493e-01,  1.0270e+00,\n",
      "         -1.9511e+00,  9.9772e-01, -1.6080e+00,  7.9333e-01, -5.7938e-01],\n",
      "        [-3.6700e-01,  4.7607e-02,  7.2560e-01,  1.0175e+00,  4.5307e-02,\n",
      "         -9.6756e-01,  3.8221e-01,  2.0819e+00,  1.6247e+00,  1.1877e+00],\n",
      "        [ 1.4165e+00, -4.2172e-01,  2.2277e-01,  9.1506e-01, -1.6754e+00,\n",
      "          8.6866e-01, -1.4126e+00,  2.4796e-01, -2.5018e-01,  1.0598e+00]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-6.1625e-01,  1.2621e-01,  3.4461e-01, -1.1111e+00,  5.9820e-01,\n",
      "          7.8933e-01, -2.7335e-01,  1.7521e-01,  2.8431e-01, -9.0703e-01],\n",
      "        [-1.7012e+00,  8.6156e-01, -1.0809e+00, -3.0783e-01,  2.1640e-01,\n",
      "         -9.6814e-01, -2.1136e-01,  9.8924e-01,  2.1264e-01,  1.1606e+00],\n",
      "        [ 3.4729e-01, -1.2512e-01, -1.2228e-01,  8.8454e-02, -5.3753e-01,\n",
      "          7.9245e-01,  1.0429e+00,  8.8542e-01, -5.5400e-01, -7.0916e-02],\n",
      "        [-2.4799e+00,  1.6774e+00,  1.0907e+00, -1.7107e+00, -1.1847e+00,\n",
      "          1.5597e+00, -1.0764e+00,  6.9514e-01,  3.1737e-01, -1.4226e+00],\n",
      "        [ 1.2127e+00, -1.0728e+00, -1.1105e+00,  8.6393e-02,  1.0513e+00,\n",
      "         -1.0606e-03, -4.3781e-01, -7.2729e-01,  1.1753e+00,  8.5497e-01],\n",
      "        [-1.5438e+00,  9.3766e-02, -1.0535e+00, -7.9392e-01,  2.7582e-01,\n",
      "         -4.0770e-01, -1.2803e-01,  1.0901e-01,  8.4144e-01, -3.9054e-02],\n",
      "        [ 2.4156e+00, -1.9828e-01,  3.6740e-01, -1.6004e+00, -5.7037e-01,\n",
      "          9.8022e-01,  5.8136e-01,  1.5566e+00, -1.0533e+00,  7.0555e-01],\n",
      "        [ 2.3271e-01,  1.0330e+00, -1.1647e+00, -1.7206e+00,  3.9171e-01,\n",
      "          1.4459e+00, -7.3664e-01,  1.6647e-02, -1.4075e+00,  5.9735e-01],\n",
      "        [ 1.7189e+00, -1.6423e-01, -1.5524e+00,  2.2126e-01, -3.5967e-01,\n",
      "         -3.4635e-01,  3.9599e-01, -5.1831e-02, -1.5424e+00, -5.4564e-01],\n",
      "        [ 1.8662e+00,  5.0110e-03,  4.4089e-01,  1.1126e+00, -2.1505e+00,\n",
      "         -2.5123e+00, -5.1982e-01, -1.2290e+00,  9.2931e-01, -1.0180e+00]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.8953, -0.3615,  0.4753,  1.1700,  1.6865,  0.2889,  0.0467, -1.4838,\n",
      "          0.3864, -1.1394],\n",
      "        [ 0.6254, -0.5747, -0.1526,  0.3167, -0.4739, -0.5440,  1.2870,  0.3937,\n",
      "          0.3574, -0.1996],\n",
      "        [-1.7616,  0.4929, -0.4316, -1.2804, -0.9766, -1.0043,  0.8449, -0.1366,\n",
      "         -0.4190, -0.5303],\n",
      "        [-1.5679, -1.1865,  1.0826,  0.4712,  1.4698, -1.7622, -0.9058, -0.0649,\n",
      "          0.6323,  0.6079],\n",
      "        [-0.8592,  1.8790, -0.2430,  0.5723,  0.4065, -0.2152, -2.4618, -0.7701,\n",
      "          2.2278,  1.8014],\n",
      "        [-0.1948, -0.3828,  0.2863, -0.5889,  1.4784,  0.1655, -0.6210,  0.8908,\n",
      "          1.5122,  0.2265],\n",
      "        [ 1.4908,  2.5312, -2.3481, -0.3187,  0.5798, -0.2416, -0.2247,  1.0257,\n",
      "          0.1547, -1.8359],\n",
      "        [-1.1590, -0.2962,  0.1947, -0.7130, -0.5469, -0.0273, -0.1015,  1.2766,\n",
      "         -1.7669,  0.2722],\n",
      "        [-0.7799, -0.3025, -0.6298, -0.9508,  0.5192, -0.8071,  0.4807, -0.0263,\n",
      "         -0.8638,  0.9969],\n",
      "        [ 1.2612,  0.2232, -0.5062,  1.7530, -1.5044, -0.4068,  0.4087, -1.1241,\n",
      "          1.4072, -1.9155]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.1849,  1.4628,  0.4263, -1.2607, -0.2344, -0.6331,  0.6598, -0.7407,\n",
      "         -1.0052,  1.4887],\n",
      "        [-0.9988,  1.3286, -1.2339, -0.6500,  0.4484, -1.4347,  1.3689,  0.2974,\n",
      "          1.3399, -0.3662],\n",
      "        [-0.0273, -0.8661,  1.4668, -0.6859,  0.2969, -0.5997, -1.0947,  0.3606,\n",
      "          1.6350, -1.6106],\n",
      "        [ 0.0934,  0.2761,  1.7350, -0.3785,  0.2140,  1.9025,  0.1135,  0.4500,\n",
      "          0.8026, -1.4971],\n",
      "        [ 2.1771,  0.8690, -0.2039, -0.9629,  1.1727,  0.1908,  0.4711,  0.4356,\n",
      "          1.1726, -0.7571],\n",
      "        [ 0.0899, -0.0131, -1.2122,  0.4358, -1.1592,  1.2774, -2.0582,  0.1428,\n",
      "         -0.6463,  0.6708],\n",
      "        [ 0.3757,  0.6638,  0.1332,  0.4544,  0.2063, -0.1641, -1.7259,  0.5366,\n",
      "          1.0138,  0.7041],\n",
      "        [ 0.6799, -0.0598,  0.5931, -1.4430,  1.3618, -0.5813,  0.3679,  0.4076,\n",
      "         -0.2435, -2.6656],\n",
      "        [ 0.2947,  0.5046, -0.8190, -0.8708, -1.0261, -0.3186, -0.7339, -1.8261,\n",
      "          0.2095,  2.1537],\n",
      "        [ 1.9666,  0.8975, -1.9077,  0.4830, -0.3991, -1.0429,  0.5483,  0.1019,\n",
      "         -2.3544, -0.4757]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2642, -0.3444,  0.5950, -0.0872,  0.4458, -2.7575,  0.0573, -1.3314,\n",
      "          0.8614, -0.5180],\n",
      "        [-1.1463, -0.7559,  0.0166,  2.0040, -0.5623, -1.2257,  0.2665,  3.0164,\n",
      "         -0.1957, -0.5092],\n",
      "        [ 0.4477, -0.3151, -0.0039,  0.8315,  0.6310, -2.1447,  1.8446,  1.3107,\n",
      "         -1.6438,  1.4086],\n",
      "        [-0.9964, -1.2353,  1.7014,  0.5486,  1.1431,  0.8909,  0.3641,  0.7960,\n",
      "          0.8083,  1.2832],\n",
      "        [ 0.6153, -0.2420,  0.7500,  0.1753,  0.4087,  0.5785,  1.4520, -0.1059,\n",
      "          0.3223, -1.0337],\n",
      "        [-1.5574, -0.0330, -0.1288, -0.3439,  1.5869, -0.0429, -1.1599, -0.6245,\n",
      "          0.9740,  0.4370],\n",
      "        [ 0.9097, -1.2936, -0.5366, -0.7112,  0.9509, -1.5670,  0.7074, -2.4378,\n",
      "          1.6322,  0.9929],\n",
      "        [-0.3286,  0.4050, -1.2534, -0.3385,  0.2062,  1.3095,  0.3685,  0.4335,\n",
      "         -0.8921,  1.2423],\n",
      "        [ 2.1918, -0.7603, -0.2864,  0.5529, -0.8751, -0.9323,  0.4536,  0.7673,\n",
      "         -0.8445, -0.5224],\n",
      "        [ 0.4152, -1.2848,  0.8075, -0.3067,  2.0015, -0.1824, -0.6912, -1.8679,\n",
      "          1.4992, -2.1620]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2985,  0.8824, -1.1359, -1.4517, -0.7907, -0.3650,  1.0809, -1.1645,\n",
      "          0.9936, -2.1087],\n",
      "        [ 0.8647,  1.3638, -0.1518,  0.9280,  0.6093, -0.0203, -0.0044,  1.9257,\n",
      "          0.7033,  0.6039],\n",
      "        [-1.6053, -1.7544, -0.3860,  0.4780,  0.5403,  1.0517,  0.4030,  0.6060,\n",
      "          0.0945,  1.9685],\n",
      "        [ 0.2789, -0.6236, -1.3192, -0.4920,  0.1109, -1.1057, -1.0543,  0.2828,\n",
      "         -0.1477,  0.0642],\n",
      "        [ 2.1203, -1.3804, -0.2800,  0.7757, -1.1101,  0.0295,  1.5840,  0.5086,\n",
      "          0.4447,  0.5156],\n",
      "        [-1.2234, -0.5190,  1.5992,  0.2295,  0.4649,  0.9425, -0.6293,  0.8505,\n",
      "         -1.1646,  0.7276],\n",
      "        [ 1.3227, -0.9602, -0.1720,  0.3007,  0.3941, -0.6629,  0.1879,  0.7954,\n",
      "          0.8328, -1.2460],\n",
      "        [-0.2349,  0.1204,  1.6443, -0.8434,  1.1284,  0.7238,  0.5486,  0.6963,\n",
      "          0.0367,  1.1421],\n",
      "        [ 0.3809,  0.2118, -1.2104, -0.1761,  0.6973, -1.7229, -0.9694,  0.8960,\n",
      "          0.6696,  0.2822],\n",
      "        [-1.6355, -0.0266, -0.2243, -0.8637, -0.2976,  0.4195,  1.8838,  1.0377,\n",
      "         -0.5521,  1.2256]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.6475,  0.1448, -1.3149, -0.6538,  0.1307, -2.1181,  2.7143,  0.1380,\n",
      "         -0.7217, -0.0205],\n",
      "        [-0.1559,  0.9849, -0.7945,  0.2474, -1.5338, -0.8518,  0.6662, -0.3753,\n",
      "         -2.0536, -0.5075],\n",
      "        [ 0.3816,  0.1937, -0.7017, -0.5466,  1.5203,  0.5516, -0.0434,  0.9076,\n",
      "          0.7695,  0.9709],\n",
      "        [ 0.3667,  0.0610, -0.5425,  0.3375, -1.9050, -0.2052,  0.2460, -0.2608,\n",
      "          1.1698, -0.7741],\n",
      "        [-0.4378,  0.6325, -0.0063,  0.4365,  0.3759,  0.2053, -1.7227,  0.1340,\n",
      "          0.9630, -1.2011],\n",
      "        [ 0.7690,  0.6705,  0.8321,  2.0639, -0.7620, -0.9627,  0.1891, -1.9215,\n",
      "         -1.8717,  1.5760],\n",
      "        [ 1.4663, -0.1906,  0.5119, -1.2398, -0.1526,  0.9062,  0.4378,  0.7603,\n",
      "          0.5178, -0.8142],\n",
      "        [ 0.9636,  0.5321, -0.6991,  0.5028, -1.4403,  0.8738,  0.6305, -0.7677,\n",
      "         -0.7680, -0.3796],\n",
      "        [-0.5355, -0.1155,  0.7426, -1.6610,  0.2673, -1.2893,  1.0259, -0.9792,\n",
      "         -1.1009, -0.3654],\n",
      "        [-1.3828, -0.0556,  1.3958,  0.2813, -1.6625, -0.1566, -0.6168,  2.4090,\n",
      "         -0.2403, -0.3722]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2883, -1.4032,  0.4502,  0.6242,  0.5449,  1.6045,  0.9756, -0.6032,\n",
      "          0.1654, -0.6475],\n",
      "        [ 0.3346, -0.4925,  0.4923,  0.9009,  1.0240, -1.6883, -0.1348,  1.0985,\n",
      "         -0.3957,  0.7819],\n",
      "        [-0.1663,  0.4264,  1.6237,  0.1083, -1.2047, -0.3563,  1.1968, -2.3732,\n",
      "          0.2369,  0.2293],\n",
      "        [ 0.4040,  1.6297, -0.9060,  0.7580,  0.1914,  1.4834, -0.8756,  0.1410,\n",
      "         -0.3708,  1.2501],\n",
      "        [ 0.4564,  0.4796,  0.5215,  0.9452, -0.6639, -1.7188, -0.4374, -1.6322,\n",
      "          1.5855, -0.3622],\n",
      "        [ 0.0175, -0.7684,  0.7442, -0.2809, -0.7543,  0.3435, -0.4012, -0.5410,\n",
      "          2.5794, -0.1917],\n",
      "        [-0.2816, -0.9468,  1.1806,  0.4056, -1.1283, -0.5540,  0.7846, -1.8846,\n",
      "          1.9531,  1.0168],\n",
      "        [-0.0404, -0.4147, -0.3952,  0.0929, -1.4738, -2.2735, -0.4540, -0.7379,\n",
      "          0.2231, -1.0234],\n",
      "        [-0.2589,  1.2767, -0.1410,  0.0271,  0.1213,  1.5833, -2.5931,  0.0823,\n",
      "         -0.1378, -0.1298],\n",
      "        [ 0.4892, -0.9601,  0.3280,  0.3091,  0.4007, -0.2795, -0.8023, -2.1374,\n",
      "          0.5736, -0.8134]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4031, -0.4414,  0.0708,  0.1120, -0.9586, -1.3190,  1.0640, -1.8061,\n",
      "          0.3118, -0.9708],\n",
      "        [-0.5046, -0.4324, -1.5011, -0.0732,  0.0639, -1.7312,  0.8574, -1.5238,\n",
      "         -0.2297,  0.1078],\n",
      "        [-0.1280, -0.5085,  0.4903,  0.1249,  0.6124,  0.3228,  0.1047, -1.2253,\n",
      "          2.2332,  0.4652],\n",
      "        [-0.2334, -0.2587,  0.2467, -2.0040, -0.9463,  0.4869,  1.0998,  0.8135,\n",
      "          0.3810, -1.2835],\n",
      "        [-0.5193, -1.2950, -1.5417,  0.4371, -1.6358, -0.9226,  1.4524, -0.1122,\n",
      "          1.4977,  1.6622],\n",
      "        [ 1.1518, -1.5075, -1.2518, -0.1894, -0.1388, -0.0027, -0.9757,  0.5833,\n",
      "         -0.2042, -0.3914],\n",
      "        [-0.1239,  2.2199, -0.4643, -0.7417,  0.1074,  0.5558,  0.8933,  2.1096,\n",
      "         -1.0923,  0.2384],\n",
      "        [ 1.8631, -0.0652,  0.7114, -0.1735, -0.1889, -0.0645, -0.8784, -0.6872,\n",
      "          0.0300, -1.2169],\n",
      "        [-0.8765,  0.5133, -0.8814,  0.1379, -0.2034, -0.0285, -0.1645,  0.4580,\n",
      "         -1.7832, -2.1368],\n",
      "        [ 1.2707, -0.2044, -0.8324, -1.9782, -0.5367, -1.1202,  0.0078,  0.1758,\n",
      "         -0.2771, -0.1600]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.6000e+00,  5.6172e-01, -1.2944e+00, -1.1119e+00,  1.2941e+00,\n",
      "         -8.3653e-01, -1.8543e-01, -9.8741e-01,  1.1576e-01,  1.2433e-01],\n",
      "        [ 1.7378e+00, -1.1372e+00,  7.1141e-02,  2.1063e-01, -1.3514e+00,\n",
      "         -4.8277e-01, -1.7696e+00, -4.4601e-01, -1.0761e+00,  4.9233e-01],\n",
      "        [-2.5181e-01,  2.2087e-01,  1.7559e+00, -9.5767e-01,  1.0733e+00,\n",
      "          1.1262e+00, -6.1205e-01,  3.0470e-01,  4.2662e-01,  2.0094e-01],\n",
      "        [-2.6311e-01,  2.4035e-01, -9.8927e-01, -1.5231e-01, -8.4115e-01,\n",
      "         -1.4364e+00, -5.0712e-01, -9.7737e-01, -1.5810e-01, -2.4023e-01],\n",
      "        [-1.3483e+00,  1.3849e+00,  1.6666e+00,  7.8293e-01, -2.7098e-01,\n",
      "          5.1353e-01,  1.3162e+00,  2.0121e-01, -3.5664e-01,  1.9006e-01],\n",
      "        [-1.3631e+00,  1.6597e-01, -7.6605e-01,  9.6770e-01, -9.3694e-01,\n",
      "         -1.0754e+00, -1.3867e-02, -5.7235e-01, -4.4938e-01, -1.9852e+00],\n",
      "        [-2.3370e+00,  3.5824e-01,  9.1210e-01, -5.4821e-01, -7.4710e-02,\n",
      "         -9.6169e-01,  2.4505e-02,  4.4475e-01,  1.4534e+00,  1.8345e+00],\n",
      "        [-1.4543e+00, -8.4877e-01,  1.7201e+00,  8.1471e-01, -1.4558e-01,\n",
      "         -4.9728e-01, -2.6294e-01, -4.2967e-01, -8.2817e-01, -2.5092e-01],\n",
      "        [-8.1629e-02,  1.7702e+00,  1.2192e+00, -3.7381e-01,  1.5084e-01,\n",
      "         -1.4329e+00, -3.2953e-01, -1.0369e+00,  5.0997e-01, -2.6648e-01],\n",
      "        [ 6.4685e-04, -7.0773e-02,  1.2814e+00,  7.6937e-02,  3.7313e-01,\n",
      "          8.6838e-01,  9.9494e-02, -2.0936e+00, -7.3206e-01,  8.1744e-01]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ParameterList can act as an iterable, or be indexed using ints\n",
    "        for i, p in enumerate(self.params):\n",
    "            x = self.params[i // 2].mm(x) + p.mm(x)\n",
    "        return x\n",
    "\n",
    "model = MyModule()\n",
    "for para in model.parameters():\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3720, -2.3720, -2.3720, -2.3720, -2.3720, -2.3720, -2.3720, -2.3720,\n",
       "         -2.3720, -2.3720],\n",
       "        [-1.2823, -1.2823, -1.2823, -1.2823, -1.2823, -1.2823, -1.2823, -1.2823,\n",
       "         -1.2823, -1.2823],\n",
       "        [-3.2999, -3.2999, -3.2999, -3.2999, -3.2999, -3.2999, -3.2999, -3.2999,\n",
       "         -3.2999, -3.2999],\n",
       "        [-3.7744, -3.7744, -3.7744, -3.7744, -3.7744, -3.7744, -3.7744, -3.7744,\n",
       "         -3.7744, -3.7744],\n",
       "        [ 3.8576,  3.8576,  3.8576,  3.8576,  3.8576,  3.8576,  3.8576,  3.8576,\n",
       "          3.8576,  3.8576]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "                'left': nn.Parameter(torch.randn(5, 10)),\n",
    "                'right': nn.Parameter(torch.randn(5, 10))\n",
    "        })\n",
    "\n",
    "    def forward(self, x, choice):\n",
    "        # torch.mm() a@b\n",
    "        # torch.mul() a*b\n",
    "        x = self.params[choice].mm(x)\n",
    "        return x\n",
    "model = MyModule()\n",
    "model(torch.ones((10,10)), 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1652]],\n",
       "\n",
       "         [[ 0.4522]],\n",
       "\n",
       "         [[-0.0740]],\n",
       "\n",
       "         [[ 0.4692]],\n",
       "\n",
       "         [[ 0.2617]],\n",
       "\n",
       "         [[-0.0157]],\n",
       "\n",
       "         [[ 0.0117]],\n",
       "\n",
       "         [[ 0.1744]],\n",
       "\n",
       "         [[ 0.2167]],\n",
       "\n",
       "         [[ 0.6162]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1652]],\n",
       "\n",
       "         [[ 0.4522]],\n",
       "\n",
       "         [[-0.0740]],\n",
       "\n",
       "         [[ 0.4692]],\n",
       "\n",
       "         [[ 0.2617]],\n",
       "\n",
       "         [[-0.0157]],\n",
       "\n",
       "         [[ 0.0117]],\n",
       "\n",
       "         [[ 0.1744]],\n",
       "\n",
       "         [[ 0.2167]],\n",
       "\n",
       "         [[ 0.6162]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1652]],\n",
       "\n",
       "         [[ 0.4522]],\n",
       "\n",
       "         [[-0.0740]],\n",
       "\n",
       "         [[ 0.4692]],\n",
       "\n",
       "         [[ 0.2617]],\n",
       "\n",
       "         [[-0.0157]],\n",
       "\n",
       "         [[ 0.0117]],\n",
       "\n",
       "         [[ 0.1744]],\n",
       "\n",
       "         [[ 0.2167]],\n",
       "\n",
       "         [[ 0.6162]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1652]],\n",
       "\n",
       "         [[ 0.4522]],\n",
       "\n",
       "         [[-0.0740]],\n",
       "\n",
       "         [[ 0.4692]],\n",
       "\n",
       "         [[ 0.2617]],\n",
       "\n",
       "         [[-0.0157]],\n",
       "\n",
       "         [[ 0.0117]],\n",
       "\n",
       "         [[ 0.1744]],\n",
       "\n",
       "         [[ 0.2167]],\n",
       "\n",
       "         [[ 0.6162]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1652]],\n",
       "\n",
       "         [[ 0.4522]],\n",
       "\n",
       "         [[-0.0740]],\n",
       "\n",
       "         [[ 0.4692]],\n",
       "\n",
       "         [[ 0.2617]],\n",
       "\n",
       "         [[-0.0157]],\n",
       "\n",
       "         [[ 0.0117]],\n",
       "\n",
       "         [[ 0.1744]],\n",
       "\n",
       "         [[ 0.2167]],\n",
       "\n",
       "         [[ 0.6162]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1652]],\n",
       "\n",
       "         [[ 0.4522]],\n",
       "\n",
       "         [[-0.0740]],\n",
       "\n",
       "         [[ 0.4692]],\n",
       "\n",
       "         [[ 0.2617]],\n",
       "\n",
       "         [[-0.0157]],\n",
       "\n",
       "         [[ 0.0117]],\n",
       "\n",
       "         [[ 0.1744]],\n",
       "\n",
       "         [[ 0.2167]],\n",
       "\n",
       "         [[ 0.6162]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1652]],\n",
       "\n",
       "         [[ 0.4522]],\n",
       "\n",
       "         [[-0.0740]],\n",
       "\n",
       "         [[ 0.4692]],\n",
       "\n",
       "         [[ 0.2617]],\n",
       "\n",
       "         [[-0.0157]],\n",
       "\n",
       "         [[ 0.0117]],\n",
       "\n",
       "         [[ 0.1744]],\n",
       "\n",
       "         [[ 0.2167]],\n",
       "\n",
       "         [[ 0.6162]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1652]],\n",
       "\n",
       "         [[ 0.4522]],\n",
       "\n",
       "         [[-0.0740]],\n",
       "\n",
       "         [[ 0.4692]],\n",
       "\n",
       "         [[ 0.2617]],\n",
       "\n",
       "         [[-0.0157]],\n",
       "\n",
       "         [[ 0.0117]],\n",
       "\n",
       "         [[ 0.1744]],\n",
       "\n",
       "         [[ 0.2167]],\n",
       "\n",
       "         [[ 0.6162]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1652]],\n",
       "\n",
       "         [[ 0.4522]],\n",
       "\n",
       "         [[-0.0740]],\n",
       "\n",
       "         [[ 0.4692]],\n",
       "\n",
       "         [[ 0.2617]],\n",
       "\n",
       "         [[-0.0157]],\n",
       "\n",
       "         [[ 0.0117]],\n",
       "\n",
       "         [[ 0.1744]],\n",
       "\n",
       "         [[ 0.2167]],\n",
       "\n",
       "         [[ 0.6162]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1652]],\n",
       "\n",
       "         [[ 0.4522]],\n",
       "\n",
       "         [[-0.0740]],\n",
       "\n",
       "         [[ 0.4692]],\n",
       "\n",
       "         [[ 0.2617]],\n",
       "\n",
       "         [[-0.0157]],\n",
       "\n",
       "         [[ 0.0117]],\n",
       "\n",
       "         [[ 0.1744]],\n",
       "\n",
       "         [[ 0.2167]],\n",
       "\n",
       "         [[ 0.6162]]]], grad_fn=<PreluBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.choices = nn.ModuleDict({\n",
    "                'conv': nn.Conv2d(10, 10, 3),\n",
    "                'pool': nn.MaxPool2d(3)\n",
    "        })\n",
    "        self.activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['prelu', nn.PReLU()]\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, choice, act):\n",
    "        x = self.choices[choice](x)\n",
    "        x = self.activations[act](x)\n",
    "        return x\n",
    "\n",
    "model = MyModule()\n",
    "model(torch.ones((10,10,3,3)), 'conv', 'prelu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.difference between nn.Sequential and nn.Modulelist\n",
    "both of them are subclasses of containers in torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential class stores sequential list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_net(\n",
      "  (seq): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class seq_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(seq_net, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "                   nn.Conv2d(1,20,5),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(20,64,5),\n",
    "                   nn.ReLU()\n",
    "                   )\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "model = seq_net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ModuleList can be used as list, all elements can be used as elements in the list, but the modules in the list are registered automatically to the whole net and the parameters are automatically put on the whole nn.Module model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modlist_net(\n",
      "  (modlist): ModuleList(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class modlist_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modlist_net, self).__init__()\n",
    "        self.modlist = nn.ModuleList([\n",
    "                       nn.Conv2d(1, 20, 5),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Conv2d(20, 64, 5),\n",
    "                       nn.ReLU()\n",
    "                       ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "model = modlist_net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff 1 : nn.ModuleList has no forward functions but nn.Sequential has default forward functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff 2 :  nn.Sequential can be named using OrderedDict but nn.ModuleList cannot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_net(\n",
      "  (seq): Sequential(\n",
      "    (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "class seq_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(seq_net, self).__init__()\n",
    "        self.seq = nn.Sequential(OrderedDict([\n",
    "                   ('conv1', nn.Conv2d(1,20,5)),\n",
    "                   ('relu1', nn.ReLU()),\n",
    "                   ('conv2', nn.Conv2d(20,64,5)),\n",
    "                   ('relu2', nn.ReLU())\n",
    "                   ]))\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "model = seq_net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff 3 : module in nn.ModuleList has no order, we can put modules in casual order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff 4 : we can use \"for\" for duplicate modules in nn.ModuleList."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modlist_net(\n",
      "  (modlist): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (5): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (6): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (7): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (8): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (9): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class modlist_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modlist_net, self).__init__()\n",
    "        self.modlist = nn.ModuleList([nn.Linear(10,10) for i in range(10)]\n",
    "\n",
    "                                    )\n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "model = modlist_net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Other APIs for nn.Module base class\n",
    "\n",
    "collect other APIs not mentioned in the above.\n",
    "\n",
    "\n",
    "train : effect Dropout & BatchNorm layers\n",
    "\n",
    "\n",
    "eval  : effect Dropout & BatchNorm layers ---> equivalent to self.train(false)\n",
    "\n",
    "\n",
    "requires_grad_ : change if autograd should record operations on parameters\n",
    "\n",
    "\n",
    "register_forward_pre_hook : be called every time before forward() is invoked\n",
    "\n",
    "\n",
    "register_forward_hook : be called every time when forward() is invoked\n",
    "\n",
    "\n",
    "named_parameters / named_buffers / named_modules / named_children \n",
    "parameters / buffers / modules / children\n",
    "\n",
    "add_module\n",
    "\n",
    "apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3697, 0.0677, 0.0544, 0.7485],\n",
      "        [0.9978, 0.9166, 0.8169, 0.5677],\n",
      "        [0.3757, 0.7005, 0.2651, 0.6418],\n",
      "        [0.9936, 0.0302, 0.4438, 0.9563]])\n",
      "Linear(in_features=2, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# when it comes to tensor we use requires_grad_() or requires_grad = False\n",
    "x = torch.rand((4,4))\n",
    "x.requires_grad_(False)\n",
    "x.requires_grad = False\n",
    "print(x)\n",
    "\n",
    "# when it comes to nn.Module we use requires_grad_() or requires_grad = False\n",
    "# this can be used for freezing parameters when fine tuning\n",
    "# because the grad would not be changed when passing requires_grad_(False) layers\n",
    "\n",
    "\n",
    "# ========= QUITE IMPORTANT ============\n",
    "# since the grad in y = None, we just skip the whole step altogether \n",
    "\n",
    "y = nn.Linear(2,2)\n",
    "y.requires_grad_(False)\n",
    "# or\n",
    "y.requires_grad = False\n",
    "print(y)\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        # nn.Parameter actually transform the torch.tensor(requires_grad=True)\n",
    "        # --> torch.tensor(requires_grad=True) and add this parameter into the orderedlist of nn.Module\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ParameterList can act as an iterable, or be indexed using ints\n",
    "        for i, p in enumerate(self.params):\n",
    "            x = self.params[i // 2].mm(x) + p.mm(x)\n",
    "        return x\n",
    "\n",
    "model = MyModule()\n",
    "x = model(torch.ones((10,10)))\n",
    "model.requires_grad_(False)\n",
    "loss = torch.sum(x)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
